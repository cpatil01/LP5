

    from keras.datasets import imdb
    (train_data,train_labels),(test_data,test_labels)=imdb.load_data(num_words=10000)

    max([max(sequence) for sequence in train_data])

    9999

    word_index=imdb.get_word_index()
    reverse_word_index=dict([(value,key)for (key, value) in word_index.items()])
    decode_review=' '.join([reverse_word_index.get(i-3,'?') for i in train_data[0]])
    decode_review

    "? this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy's that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all"

    import numpy as np
    def vectorize_sequences(sequences,dimension=10000):
        results=np.zeros((len(sequences),dimension))
        for i,sequence in enumerate(sequences):
            results[i,sequence]=1
        return results
    x_train=vectorize_sequences(train_data)
    x_test=vectorize_sequences(test_data)

    x_train[0]

    array([0., 1., 1., ..., 0., 0., 0.])

    x_train.shape

    (25000, 10000)

    y_train=np.asarray(train_labels).astype('float32')
    y_test=np.asarray(test_labels).astype('float')

    from keras import models
    from keras import layers

    model=models.Sequential()
    model.add(layers.Dense(16,activation='relu',input_shape=(10000,)))
    model.add(layers.Dense(16,activation='relu'))
    model.add(layers.Dense(1,activation='sigmoid'))

    from keras import optimizers
    from keras import losses
    from keras import metrics
    model.complie(optimizer=optimers.RMSprop(lr=0.001),
                 loss=losses.binary_crossentropy,
                 metrics=[metrics.binary_accuracy])

    C:\Users\Home\anaconda3\lib\site-packages\keras\optimizers\optimizer_v2\rmsprop.py:140: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.
      super().__init__(name, **kwargs)

    x_val=x_train[:10000]
    practical_x_train=x_train[10000:]

    y_val=y_train[:10000]
    practical_y_train=y_train[10000:]

    history = model.fit(practical_x_train,
                       practical_y_train,
                       epochs=30,
                       batch_size=512,
                       validation_data=(x_val,y_val))

    Epoch 1/30
    30/30 [==============================] - 24s 637ms/step - loss: 0.5322 - binary_accuracy: 0.7805 - val_loss: 0.3969 - val_binary_accuracy: 0.8660
    Epoch 2/30
    30/30 [==============================] - 1s 42ms/step - loss: 0.3135 - binary_accuracy: 0.9009 - val_loss: 0.3094 - val_binary_accuracy: 0.8829
    Epoch 3/30
    30/30 [==============================] - 1s 38ms/step - loss: 0.2228 - binary_accuracy: 0.9279 - val_loss: 0.2950 - val_binary_accuracy: 0.8816
    Epoch 4/30
    30/30 [==============================] - 1s 37ms/step - loss: 0.1749 - binary_accuracy: 0.9437 - val_loss: 0.2767 - val_binary_accuracy: 0.8908
    Epoch 5/30
    30/30 [==============================] - 1s 35ms/step - loss: 0.1421 - binary_accuracy: 0.9555 - val_loss: 0.2841 - val_binary_accuracy: 0.8876
    Epoch 6/30
    30/30 [==============================] - 1s 35ms/step - loss: 0.1216 - binary_accuracy: 0.9610 - val_loss: 0.3037 - val_binary_accuracy: 0.8820
    Epoch 7/30
    30/30 [==============================] - 1s 38ms/step - loss: 0.0931 - binary_accuracy: 0.9743 - val_loss: 0.3160 - val_binary_accuracy: 0.8824
    Epoch 8/30
    30/30 [==============================] - 1s 36ms/step - loss: 0.0810 - binary_accuracy: 0.9773 - val_loss: 0.3456 - val_binary_accuracy: 0.8764
    Epoch 9/30
    30/30 [==============================] - 1s 34ms/step - loss: 0.0650 - binary_accuracy: 0.9831 - val_loss: 0.3888 - val_binary_accuracy: 0.8725
    Epoch 10/30
    30/30 [==============================] - 1s 34ms/step - loss: 0.0546 - binary_accuracy: 0.9863 - val_loss: 0.3794 - val_binary_accuracy: 0.8768
    Epoch 11/30
    30/30 [==============================] - 1s 34ms/step - loss: 0.0416 - binary_accuracy: 0.9911 - val_loss: 0.4170 - val_binary_accuracy: 0.8744
    Epoch 12/30
    30/30 [==============================] - 1s 39ms/step - loss: 0.0352 - binary_accuracy: 0.9910 - val_loss: 0.4442 - val_binary_accuracy: 0.8740
    Epoch 13/30
    30/30 [==============================] - 1s 35ms/step - loss: 0.0264 - binary_accuracy: 0.9953 - val_loss: 0.4676 - val_binary_accuracy: 0.8737
    Epoch 14/30
    30/30 [==============================] - 1s 36ms/step - loss: 0.0230 - binary_accuracy: 0.9953 - val_loss: 0.4980 - val_binary_accuracy: 0.8721
    Epoch 15/30
    30/30 [==============================] - 1s 35ms/step - loss: 0.0148 - binary_accuracy: 0.9987 - val_loss: 0.5379 - val_binary_accuracy: 0.8707
    Epoch 16/30
    30/30 [==============================] - 1s 35ms/step - loss: 0.0150 - binary_accuracy: 0.9973 - val_loss: 0.5683 - val_binary_accuracy: 0.8698
    Epoch 17/30
    30/30 [==============================] - 1s 36ms/step - loss: 0.0105 - binary_accuracy: 0.9987 - val_loss: 0.6044 - val_binary_accuracy: 0.8668
    Epoch 18/30
    30/30 [==============================] - 1s 35ms/step - loss: 0.0089 - binary_accuracy: 0.9985 - val_loss: 0.6395 - val_binary_accuracy: 0.8668
    Epoch 19/30
    30/30 [==============================] - 1s 35ms/step - loss: 0.0043 - binary_accuracy: 0.9999 - val_loss: 0.6703 - val_binary_accuracy: 0.8673
    Epoch 20/30
    30/30 [==============================] - 1s 34ms/step - loss: 0.0067 - binary_accuracy: 0.9989 - val_loss: 0.7042 - val_binary_accuracy: 0.8693
    Epoch 21/30
    30/30 [==============================] - 1s 35ms/step - loss: 0.0025 - binary_accuracy: 0.9999 - val_loss: 0.7471 - val_binary_accuracy: 0.8668
    Epoch 22/30
    30/30 [==============================] - 1s 34ms/step - loss: 0.0042 - binary_accuracy: 0.9993 - val_loss: 0.7797 - val_binary_accuracy: 0.8669
    Epoch 23/30
    30/30 [==============================] - 1s 35ms/step - loss: 0.0014 - binary_accuracy: 0.9999 - val_loss: 0.8129 - val_binary_accuracy: 0.8641
    Epoch 24/30
    30/30 [==============================] - 1s 34ms/step - loss: 0.0054 - binary_accuracy: 0.9983 - val_loss: 0.8530 - val_binary_accuracy: 0.8652
    Epoch 25/30
    30/30 [==============================] - 1s 34ms/step - loss: 8.8799e-04 - binary_accuracy: 0.9999 - val_loss: 0.8673 - val_binary_accuracy: 0.8653
    Epoch 26/30
    30/30 [==============================] - 1s 33ms/step - loss: 7.8539e-04 - binary_accuracy: 0.9999 - val_loss: 0.9171 - val_binary_accuracy: 0.8632
    Epoch 27/30
    30/30 [==============================] - 1s 35ms/step - loss: 0.0031 - binary_accuracy: 0.9991 - val_loss: 0.9443 - val_binary_accuracy: 0.8644
    Epoch 28/30
    30/30 [==============================] - 1s 35ms/step - loss: 4.2952e-04 - binary_accuracy: 0.9999 - val_loss: 0.9656 - val_binary_accuracy: 0.8634
    Epoch 29/30
    30/30 [==============================] - 1s 36ms/step - loss: 3.5359e-04 - binary_accuracy: 0.9999 - val_loss: 1.0063 - val_binary_accuracy: 0.8631
    Epoch 30/30
    30/30 [==============================] - 1s 33ms/step - loss: 0.0044 - binary_accuracy: 0.9987 - val_loss: 1.0542 - val_binary_accuracy: 0.8624

    history_dict=history.history
    history_dict.keys()

    dict_keys(['loss', 'binary_accuracy', 'val_loss', 'val_binary_accuracy'])

    import matplotlib.pyplot as plt
    %matplotlib inline

    loss_values=history_dict['loss']
    val_loss_val=history_dict['val_loss']
    epochs=range(1,len(loss_values)+1)
    plt.plot(epochs,loss_values,'bo',label="Training Loss")
    plt.plot(epochs,val_loss_val,'b',label="Validation Loss")
    plt.title('Training and validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss Value')
    plt.legend()
    plt.show()

[]

    np.set_printoptions(suppress=True)
    result=model.predict(x_test)

    782/782 [==============================] - 5s 3ms/step

    result

    array([[0.00004066],
           [1.        ],
           [0.99961764],
           ...,
           [0.00003879],
           [0.00562141],
           [0.9392677 ]], dtype=float32)

    y_pred=np.zeros(len(result))
    for i, score in enumerate(result):
        y_pred[i]=1 if score > 0.5 else 0

    from sklearn.metrics import mean_absolute_error
    mae= mean_absolute_error(y_pred , y_test)
    mae

    0.15348
